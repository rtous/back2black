#include <stdio.h>  
#include <onnxruntime_cxx_api.h>
#include <opencv2/opencv.hpp> 
#include <opencv2/core/utils/filesystem.hpp>
#include <print>
#include <iostream> 
#include "onnxruntime_utils.h" 
#include "tensor_utils.h" 
#include "sam21.h" 

#include <random>
#include <algorithm>  // for std::clamp

SAM21::SAM21(std::string path, std::string model_type) : //model_type = t or l
    memory_info(Ort::MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU)),
    img_encoder(OrtModel("img_encoder", path+"/image_encoder_hiera_"+model_type+"_2.1.onnx")),
    prompt_encoder(OrtModel("prompt_encoder", path+"/prompt_encoder_hiera_"+model_type+"_2.1.onnx")),
    img_decoder(OrtModel("img_decoder", path+"/mask_decoder_hiera_"+model_type+"_2.1.onnx")),
    mlp(OrtModel("mlp_hiera", path+"/mlp_hiera_"+model_type+"_2.1.onnx")),
    mem_encoder(OrtModel("mem_encoder", path+"/memory_encoder_hiera_"+model_type+"_2.1.onnx")),
    mem_attention(OrtModel("mem_attention", path+"/memory_attention_hiera_"+model_type+"_2.1.opt.onnx")),
    obj_ptr_tpos_proj_hiera(OrtModel("obj_ptr_tpos_proj_hiera", path+"/obj_ptr_tpos_proj_hiera_"+model_type+"_2.1.onnx"))
{

}

inline void drawPromptPoints(cv::Mat& overlay,
                      const std::vector<PromptPoint>& points,
                      const cv::Mat& originalFrame)
{
    float scale_x_to_frame = (float)originalFrame.cols / 1024.0f;
    float scale_y_to_frame = (float)originalFrame.rows / 1024.0f;

    for (const auto& p : points) {
        int x = static_cast<int>(p.x);
        int y = static_cast<int>(p.y);

        cv::Scalar color = (p.label == 1) ? cv::Scalar(0,255,0)   // green
                                          : cv::Scalar(0,0,255);  // red

        cv::circle(overlay, cv::Point(x, y), 6, color, -1, cv::LINE_AA);
        cv::putText(overlay, (p.label == 1 ? "P" : "N"),
                    cv::Point(x+8, y-8),
                    cv::FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv::LINE_AA);
    }
}

inline cv::Mat overlayMask(const cv::Mat& image, const cv::Mat& maskProb, 
                    const cv::Scalar& color = cv::Scalar(0,0,255), // red (BGR)
                    double alpha = 0.5) 
{
    CV_Assert(image.type() == CV_8UC3);
    CV_Assert(maskProb.type() == CV_32FC1 || maskProb.type() == CV_8UC1);
    CV_Assert(image.size() == maskProb.size());

    // threshold if needed
    cv::Mat mask;
    if (maskProb.type() == CV_32FC1) {
        cv::threshold(maskProb, mask, 0.5, 1.0, cv::THRESH_BINARY);
        mask.convertTo(mask, CV_8UC1, 255.0); // 0 or 255
    } else {
        mask = maskProb.clone();
        if (mask.depth() != CV_8U) mask.convertTo(mask, CV_8UC1);
    }

    // make a 3-channel color mask
    cv::Mat maskColor(image.size(), CV_8UC3, color);
    
    // apply mask to color
    cv::Mat colored;
    maskColor.copyTo(colored, mask);

    // blend
    cv::Mat blended;
    cv::addWeighted(image, 1.0, colored, alpha, 0.0, blended);

    return blended;
}

//Generated by ChatGPT (required by make_mask_for_memory_from_logits )
inline float sigmoidf(float x){ return 1.0f / (1.0f + std::exp(-x)); }

//Generated by ChatGPT (required to prepare inputs for the memory encoder)
void make_mask_for_memory_from_logits(

    const float* lowres_logits_256x256,   // points to chosen mask (logits)
    int I,                                // 1024
    std::vector<float>& out_mask_B1I2I2,  // filled as [1,1,I,I]
    bool binarize=false,                  // true if you want hard mask for point prompts
    float scale=20.0f, float bias=-10.0f     // float scale=1.0f, float bias=0.0f
){
    // 1) resize 256x256 logits -> 1024x1024 logits
    cv::Mat m256(256, 256, CV_32FC1, const_cast<float*>(lowres_logits_256x256));
    cv::Mat mI; 
    cv::resize(m256, mI, cv::Size(I, I), 0, 0, cv::INTER_LINEAR); // still logits

    // 2) logits -> probs (or binarize)
    out_mask_B1I2I2.resize(1 * 1 * I * I);
    float* dst = out_mask_B1I2I2.data();
    for (int y=0; y<I; ++y){
        const float* row = mI.ptr<float>(y);
        for (int x=0; x<I; ++x){
            float v = row[x];
            //float p = binarize ? (v > 0.0f ? 1.0f : 0.0f) : sigmoidf(v);
            //p = p * scale + bias;
            float p;
            //fix suggested but no effective change noticed.
            if (binarize) {
                p = (v > 0.0f ? 1.0f : 0.0f);
            } else {
                float z = scale * v + bias;
                p = 1.0f / (1.0f + expf(-z)); // scaled sigmoid
            }
            dst[y * I + x] = p; // NCHW with N=C=1 means we can write row-major directly
        }
    }
}

void SAM21::reset_memory() {
    // Clear memory bank (per-frame memory of masks/positional encodings)
    inference_state.maskmem_features_BC64x64.clear();
    inference_state.maskmem_posenc_BC64x64.clear();

    // Clear prompt encoder outputs
    inference_state.prompt_encoder_out_dense_pe = TensorCopy();
    inference_state.prompt_encoder_out_sparse_embeddings = TensorCopy();
    inference_state.prompt_encoder_out_dense_embeddings = TensorCopy();

    // Clear object pointer projections
    inference_state.obj_ptr_tpos_proj_hiera_out.clear();

    // Reset image encoder state (they will be recomputed per frame anyway)
    inference_state.vision_features = TensorCopy();
    inference_state.backbone_fpn_0 = TensorCopy();
    inference_state.backbone_fpn_1 = TensorCopy();
    inference_state.dense_pe       = TensorCopy();

    // Reset score index
    inference_state.maxScoreIdx = 0;

    // Optional: also clear saved embeddings if you are caching whole videos
    //video_frames_embeddings.clear();

    printf("SAM21 memory reset. Ready for new prompts.\n");
}
 

//normalize image before encoding it
void preprocess(cv::Mat &image, std::vector<cv::Mat> &input_images){
    cv::Mat image_ = image.clone();
    // cv::subtract(image, cv::Scalar(0.406, 0.456, 0.485), image_);
    // cv::divide(image_, cv::Scalar(0.225, 0.224, 0.229), image_);
    std::vector<cv::Mat> mats{image_};
    //OpenCV function to prepare image for an NN. Mat into a 4-dimensional array/blob
    cv::Mat blob = cv::dnn::blobFromImages(mats, 1/255.0,cv::Size(1024,1024), cv::Scalar(0, 0, 0), true, false);
    input_images.clear();
    //Add preprocessed frame to input_images
    input_images.emplace_back(blob);
}

void SAM21::image_encoder(cv::Mat image)
{
  //3) preprocess input image (opencv to onnx format)
  printf("Preprocessing input image...\n");
  std::vector<cv::Mat> input_images;
  preprocess(image, input_images); //Add image to the vector (as is the first will be in pos [0])
  
  /////////////////
  // IMAGE ENCODER
  /////////////////
  //current = ailia (works with ailia and with ryouchinsa)
  //ailia:
  //input_name= [input_image] ===> [1,3,1024,1024]
  //output_name= [vision_features] ==> [1,256,64,64]
  //output_name= [vision_pos_enc_0] ==> [1,256,256,256]
  //output_name= [vision_pos_enc_1] ==> [1,256,128,128]
  //output_name= [vision_pos_enc_2] ==> [1,256,64,64]
  //output_name= [backbone_fpn_0] ==> [1,32,256,256]
  //output_name= [backbone_fpn_1] ==> [1,64,128,128]
  //output_name= [backbone_fpn_2] ==> [1,256,64,64]

  printf("/*********** image encoder ***********/\n");

  //PREPARE INPUT TENSORS (image encoder)
  printf("Preparing input tensors (image encoder)...\n");
  std::vector<Ort::Value> img_encoder_input_tensor;
  img_encoder_input_tensor.push_back(std::move(Ort::Value::CreateTensor<float>(
      memory_info,
      input_images[0].ptr<float>(),
      input_images[0].total(),
      img_encoder.inputs[img_encoder.inputIdxByName(const_cast<char*>("input_image"))].shape.data(),
      img_encoder.inputs[img_encoder.inputIdxByName(const_cast<char*>("input_image"))].shape.size()))
  );

  //RUN INFERENCE (image encoder)
  printf("Inference (image encoder)...\n");
  std::vector<Ort::Value> img_encoder_out  = img_encoder.run(img_encoder_input_tensor);
  
  //inference_state.img_encoder_out  = img_encoder.run(img_encoder_input_tensor);
  
  // store only the needed outputs as TensorCopy
  inference_state.vision_features = setTensorCopy(
    std::move(img_encoder_out[img_encoder.outputIdxByName(const_cast<char*>("vision_features"))]));
  inference_state.backbone_fpn_0 = setTensorCopy(
    std::move(img_encoder_out[img_encoder.outputIdxByName(const_cast<char*>("backbone_fpn_0"))]));
  inference_state.backbone_fpn_1 = setTensorCopy(
    std::move(img_encoder_out[img_encoder.outputIdxByName(const_cast<char*>("backbone_fpn_1"))]));
  inference_state.dense_pe = setTensorCopy(
    std::move(img_encoder_out[img_encoder.outputIdxByName(const_cast<char*>("vision_pos_enc_2"))]));   
}

cv::Mat SAM21::inference_frame(cv::Mat image, 
                    int frame_num,
                    std::vector<PromptPoint> promptPoints,
                    bool keep_memory, bool use_precoputed_embedding_vector)
{
  //if video processing
  //we expect the frame embedding already precomputed so we need to:
  if (use_precoputed_embedding_vector) { 
    inference_state.vision_features = video_frames_embeddings[frame_num].vision_features;
    inference_state.backbone_fpn_0 = video_frames_embeddings[frame_num].backbone_fpn_0;
    inference_state.backbone_fpn_1 = video_frames_embeddings[frame_num].backbone_fpn_1;
    inference_state.dense_pe = video_frames_embeddings[frame_num].dense_pe;
  }
  
  int ori_img_cols = image.size[1];
  int ori_img_rows = image.size[0];
  /*
  //3) preprocess input image (opencv to onnx format)
  printf("Preprocessing input image...\n");
  std::vector<cv::Mat> input_images;
  preprocess(image, input_images); //Add image to the vector (as is the first will be in pos [0])
  
  
  /////////////////
  // IMAGE ENCODER
  /////////////////
  //current = ailia (works with ailia and with ryouchinsa)
  //ailia:
  //input_name= [input_image] ===> [1,3,1024,1024]
  //output_name= [vision_features] ==> [1,256,64,64]
  //output_name= [vision_pos_enc_0] ==> [1,256,256,256]
  //output_name= [vision_pos_enc_1] ==> [1,256,128,128]
  //output_name= [vision_pos_enc_2] ==> [1,256,64,64]
  //output_name= [backbone_fpn_0] ==> [1,32,256,256]
  //output_name= [backbone_fpn_1] ==> [1,64,128,128]
  //output_name= [backbone_fpn_2] ==> [1,256,64,64]

  printf("--------- image encoder ---------\n");

  //PREPARE INPUT TENSORS (image encoder)
  printf("Preparing input tensors (image encoder)...\n");
  std::vector<Ort::Value> img_encoder_input_tensor;
  img_encoder_input_tensor.push_back(std::move(Ort::Value::CreateTensor<float>(
      memory_info,
      input_images[0].ptr<float>(),
      input_images[0].total(),
      img_encoder.inputs[img_encoder.inputIdxByName(const_cast<char*>("input_image"))].shape.data(),
      img_encoder.inputs[img_encoder.inputIdxByName(const_cast<char*>("input_image"))].shape.size()))
  );

  //RUN INFERENCE (image encoder)
  printf("Inference (image encoder)...\n");
  std::vector<Ort::Value> img_encoder_out  = img_encoder.run(img_encoder_input_tensor);

  //Save tensors that need to be reuse more than once:
  //[vision_features]
  int img_encoder_out_vision_features_idx = img_encoder.outputIdxByName(const_cast<char*>("vision_features"));
  TensorCopy img_encoder_out_vision_features = setTensorCopy(std::move(img_encoder_out[img_encoder_out_vision_features_idx]));  
  */

  //image_encoder(image);

  //Save tensors that need to be reuse more than once:
  //[vision_features]
  //int img_encoder_out_vision_features_idx = img_encoder.outputIdxByName(const_cast<char*>("vision_features"));
  //TensorCopy img_encoder_out_vision_features = setTensorCopy(std::move(img_encoder_out[img_encoder_out_vision_features_idx]));  
  
  //TensorCopy img_encoder_out_vision_features = setTensorCopy(std::move(inference_state.img_encoder_out[img_encoder_out_vision_features_idx]));  
  
  //TensorCopy img_encoder_out_vision_features = getTensorCopy(inference_state.vision_features, memory_info);  
  



  //////////////////////
  // PROMPT ENCODER 
  /////////////////////
  //alia (no in ryouchinsa, integrated in the decoder)
  //input_name= [coords] ===> [-1,-1,2] ===> float32
  //input_name= [labels] ===> [-1,-1] ===> int32
  //input_name= [masks] ===> [-1,-1,-1] ===> float32
  //input_name= [masks_enable] ===> [1] ===> int32
  //output_name= [sparse_embeddings] ===> [-1,-1,256] ===> float32
  //output_name= [dense_embeddings] ===> [-1,256,-1,-1] ===> float32
  //output_name= [dense_pe] ===> [1,256,64,64] ===> float32

  

  if (frame_num == 0)
  {
    printf("/*********** prompt encoder ***********/\n");
    
    //PREPARE INPUT TENSORS (prompt encoder)
    printf("Preparing input tensors (prompt encoder)...\n");
    std::vector<Ort::Value> prompt_encoder_input_tensor;

    

    /*inputPointValues.push_back((float)210);
    inputPointValues.push_back((float)350);
    inputLabelValues.push_back(1);
    inputPointValues.push_back((float)250);
    inputPointValues.push_back((float)220);
    inputLabelValues.push_back(1);*/

    std::vector<float> inputPointValues;
    std::vector<int> inputLabelValues;

    // Scale to model space (assume model input is 1024x1024)
    float scale_x_to_model = 1024.0f / (float)image.cols;
    float scale_y_to_model = 1024.0f / (float)image.rows;

    for (const auto& p : promptPoints) {
        inputPointValues.push_back(p.x * scale_x_to_model);
        inputPointValues.push_back(p.y * scale_y_to_model);
        inputLabelValues.push_back(p.label);
    }

    int numPoints = (int)inputLabelValues.size();
    int batchNum = 1;

    //scale coord to SAM2 size (1024x1024):
    /*
    float scale_x_to_model = 1024.0f / (float)image.cols;
    float scale_y_to_model = 1024.0f / (float)image.rows;
    for (int i=0; i<numPoints; i++) {
        inputPointValues[i*2]   = inputPointValues[i*2]   * scale_x_to_model;
        inputPointValues[i*2+1] = inputPointValues[i*2+1] * scale_y_to_model;
    }*/


    std::vector<int64_t> inputPointShape = {batchNum, numPoints, 2};
    std::vector<int64_t> inputLabelShape = {batchNum, numPoints};
    prompt_encoder_input_tensor.push_back(Ort::Value::CreateTensor<float>(memory_info, inputPointValues.data(), 2 * numPoints * batchNum, inputPointShape.data(), inputPointShape.size()));
    prompt_encoder_input_tensor.push_back(Ort::Value::CreateTensor<int32_t>(memory_info, inputLabelValues.data(), numPoints * batchNum, inputLabelShape.data(), inputLabelShape.size()));

    //masks
    const size_t maskInputSize = 256 * 256;
    std::vector<float> previousMaskInputValues;
    float maskInputValues[maskInputSize];
    memset(maskInputValues, 0, sizeof(maskInputValues));
    int hasMaskValues[] = {0};
    std::vector<int64_t> maskInputShape = {1, 256, 256},
    hasMaskInputShape = {1};
    if(hasMaskValues[0] == 1)
    {
      prompt_encoder_input_tensor.push_back(Ort::Value::CreateTensor<float>(memory_info, previousMaskInputValues.data(), maskInputSize, maskInputShape.data(), maskInputShape.size()));
    } else{
      prompt_encoder_input_tensor.push_back(Ort::Value::CreateTensor<float>(memory_info, maskInputValues, maskInputSize, maskInputShape.data(), maskInputShape.size()));
    }

    //masks_enable
    prompt_encoder_input_tensor.push_back(Ort::Value::CreateTensor<int32_t>(memory_info, hasMaskValues, 1, hasMaskInputShape.data(), hasMaskInputShape.size()));

    //RUN INFERENCE (prompt encoder)
    printf("Inference (prompt encoder)...\n");
    std::vector<Ort::Value> prompt_encoder_out  = prompt_encoder.run(prompt_encoder_input_tensor);

    //Only need to run this for the first frame, so storing copies of the outputs:
    //[dense_pe]
    int prompt_encoder_out_dense_pe_idx = prompt_encoder.outputIdxByName(const_cast<char*>("dense_pe"));
    inference_state.prompt_encoder_out_dense_pe = setTensorCopy(std::move(prompt_encoder_out[prompt_encoder_out_dense_pe_idx]));
    
    //[sparse_prompt_embeddings]
    int prompt_encoder_out_sparse_embeddings_idx = prompt_encoder.outputIdxByName(const_cast<char*>("sparse_embeddings"));
    inference_state.prompt_encoder_out_sparse_embeddings = setTensorCopy(std::move(prompt_encoder_out[prompt_encoder_out_sparse_embeddings_idx]));
    
    //[dense_prompt_embeddings]
    int prompt_encoder_out_dense_embeddings_idx = prompt_encoder.outputIdxByName(const_cast<char*>("dense_embeddings"));
    inference_state.prompt_encoder_out_dense_embeddings = setTensorCopy(std::move(prompt_encoder_out[prompt_encoder_out_dense_embeddings_idx]));
  }

  //////////////////////
  // MEM ATTENTION
  /////////////////////
  //alia 
  //input_name= [curr] ===> [4096,1,256] ===> float32
  //input_name= [memory_1] ===> [-1,1,64] ===> float32
  //input_name= [memory_2] ===> [-1,1,64] ===> float32
  //input_name= [curr_pos] ===> [4096,1,256] ===> float32
  //input_name= [memory_pos_1] ===> [-1,1,64] ===> float32
  //input_name= [memory_pos_2] ===> [-1,1,64] ===> float32
  //input_name= [attention_mask_1] ===> [-1,1] ===> bool
  //input_name= [attention_mask_2] ===> [-1,1] ===> bool
  //output_name= [pix_feat] ===> [4096,1,256] ===> float32

  TensorCopy pix_feat_nchw_for_decoder;
  if (frame_num > 0)
  {
      printf("/************ mem_attention *************/\n");
      
      //PREPARE INPUT TENSORS (mem_attention)
      printf("Preparing input tensors (mem_attention)...\n");
      std::vector<Ort::Value> mem_attention_input_tensor;

      //From image incoder ouputs:
      //output_name= [vision_features] ==> [1,256,64,64]
      //output_name= [vision_pos_enc_0] ==> [1,256,256,256]
      //output_name= [vision_pos_enc_1] ==> [1,256,128,128]
      //output_name= [vision_pos_enc_2] ==> [1,256,64,64]
      //output_name= [backbone_fpn_0] ==> [1,32,256,256]
      //output_name= [backbone_fpn_1] ==> [1,64,128,128]
      //output_name= [backbone_fpn_2] ==> [1,256,64,64]

      //[curr] ===> [4096,1,256] ===> float32
      //img_decoder_input_tensor.push_back(getTensorCopy(img_encoder_out_vision_features, memory_info));
      //[memory_1] ===> [-1,1,64] ===> float32
      //[memory_2] ===> [-1,1,64] ===> float32
      //[curr_pos] ===> [4096,1,256] ===> float32
      //[memory_pos_1] ===> [-1,1,64] ===> float32
      //[memory_pos_2] ===> [-1,1,64] ===> float32
      //[attention_mask_1] ===> [-1,1] ===> bool
      //[attention_mask_2] ===> [-1,1] ===> bool

      //flattens current vision features and dense_pe,
      //flattens & concatenates all saved frames from inference_state.maskmem_*,
      //makes safe masks,
      //adds a one-row dummy memory_2 token (masked out) for now (you can swap it later for obj-ptr tokens),
      //runs mem_attention,
      //prepares pix_feat_nchw_for_decoder for the decoder.

      // 1) curr and curr_pos from this frame
      printf("\tcurr and curr_pos...\n");
      auto curr_seq     = flatten_nchw_to_hw1c(inference_state.vision_features);           // [4096,1,256]
      auto curr_pos_seq = flatten_nchw_to_hw1c(inference_state.prompt_encoder_out_dense_pe); // [4096,1,256]

      // 2) memory_1 and memory_pos_1 from the memory bank
      printf("\tmemory_1 and memory_pos_1 from the memory bank (and attention_mask_1)...\n");
      std::vector<TensorCopy> mem1_list, mem1pos_list;
      for (size_t k=0;k<inference_state.maskmem_features_BC64x64.size();++k) {
        auto m  = flatten_nchw_to_hw1c(inference_state.maskmem_features_BC64x64[k]); // [4096,1,64]
        auto mp = flatten_nchw_to_hw1c(inference_state.maskmem_posenc_BC64x64[k]); // [4096,1,64]
        mem1_list.push_back(std::move(m));
        mem1pos_list.push_back(std::move(mp));
      }
      auto memory_1     = concat_seq_axis0(mem1_list);     // [-1,1,64]
      auto memory_pos_1 = concat_seq_axis0(mem1pos_list);  // [-1,1,64]

      // attention_mask_1 : all true if we have memory, else a single false
      int64_t len_mem1 = getTensorCopy(memory_1, memory_info).GetTensorTypeAndShapeInfo().GetShape()[0];
      bool have_mem1 = !inference_state.maskmem_features_BC64x64.empty();
      auto attention_mask_1 = make_bool_mask(len_mem1, have_mem1);

      // 3) memory_2 and memory_pos_2 (object-pointer path) — placeholder bootstrap
      //    For now: provide a single dummy row with mask=false.
      //    Later you can project MLP tokens with obj_ptr_tpos_proj_hiera and concat here.
      /*
      //Without the output of MLP->obj_ptr_tpos_proj_hiera
      printf("\tmemory_2 and memory_pos_2 (and attention_mask_2)...\n");
      std::vector<int64_t> shape_dummy{1,1,64};
      std::vector<float> zeros64(64, 0.0f);
      TensorCopy memory_2 = setTensorCopy(
        Ort::Value::CreateTensor<float>(memory_info, zeros64.data(), zeros64.size(), shape_dummy.data(), shape_dummy.size()));
      TensorCopy memory_pos_2 = setTensorCopy(
      Ort::Value::CreateTensor<float>(memory_info, zeros64.data(), zeros64.size(), shape_dummy.data(), shape_dummy.size()));
      auto attention_mask_2 = make_bool_mask(1, false, memory_info);
      */

      //Using obj_ptr_tpos_proj_hiera
      // Single output: [N,64] (features only)
      //auto& ptr_feat_val = obj_ptr_proj_out[0];
      auto& ptr_feat_val = inference_state.obj_ptr_tpos_proj_hiera_out[0];
      //auto& ptr_feat_val = getTensorCopy(inference_state.obj_ptr_tpos_proj_hiera_out_x_out, memory_info);
      auto shape_ptr = ptr_feat_val.GetTensorTypeAndShapeInfo().GetShape(); // [N,64]
      int Nptr = (int)shape_ptr[0];
      const float* fptr = ptr_feat_val.GetTensorData<float>();
      
      // Build a vector of [1,1,64] tensors (one per pointer token)
      std::vector<TensorCopy> ptr_feats, ptr_poses;
      std::vector<int64_t> shape{1,1,64};
      for (int i=0; i<Nptr; ++i) {
        std::vector<float> token(fptr + i*64, fptr + (i+1)*64); // copy 64 floats

        // feature embedding
        ptr_feats.push_back(setTensorCopy(
            Ort::Value::CreateTensor<float>(memory_info, token.data(), token.size(), shape.data(), shape.size())));
        //ptr_feats.push_back(getTensorCopy(inference_state.inference_state.obj_ptr_tpos_proj_hiera_out_x_out, memory_info));

        // positional embedding = zeros (fallback since ONNX has no pos output)
        std::vector<float> zeros64(64, 0.0f);
        ptr_poses.push_back(setTensorCopy(
            Ort::Value::CreateTensor<float>(memory_info, zeros64.data(), zeros64.size(),
                                            shape.data(), shape.size())));
      }
      // Concatenate all object-pointer rows
      auto memory_2     = concat_seq_axis0(ptr_feats);   // [-1,1,64]
      auto memory_pos_2 = concat_seq_axis0(ptr_poses);   // [-1,1,64]
      auto attention_mask_2 = make_bool_mask(Nptr, true);

      // 4) Pack inputs and run     
      mem_attention_input_tensor.push_back(getTensorCopy(curr_seq, memory_info));       // curr     
      mem_attention_input_tensor.push_back(getTensorCopy(memory_1, memory_info));       // memory_1     
      mem_attention_input_tensor.push_back(getTensorCopy(memory_2, memory_info));       // memory_2     
      mem_attention_input_tensor.push_back(getTensorCopy(curr_pos_seq, memory_info));   // curr_pos     
      mem_attention_input_tensor.push_back(getTensorCopy(memory_pos_1, memory_info));   // memory_pos_1     
      mem_attention_input_tensor.push_back(getTensorCopy(memory_pos_2, memory_info));   // memory_pos_2      
      mem_attention_input_tensor.push_back(getTensorCopy(attention_mask_1, memory_info)); // attention_mask_1 (bool) 
      mem_attention_input_tensor.push_back(getTensorCopy(attention_mask_2, memory_info)); // attention_mask_2 (bool)

      //RUN INFERENCE (mem_attention encoder)
      printf("Inference (mem_attention)...\n");
      std::vector<Ort::Value> mem_attention_out  = mem_attention.run(mem_attention_input_tensor);

      // 5) mem-conditioned pixel features back to NCHW for the decoder
      int mem_attn_pix_idx = mem_attention.outputIdxByName(const_cast<char*>("pix_feat")); // [4096,1,256]
      TensorCopy pix_feat_seq = setTensorCopy(std::move(mem_attention_out[mem_attn_pix_idx]));
      pix_feat_nchw_for_decoder = reshape_hw1c_to_nchw_64x64(pix_feat_seq);

      //RUN INFERENCE (mem_attention encoder)
      //printf("Inference (mem_attention)...\n");
      //std::vector<Ort::Value> mem_attention_out  = mem_attention.run(mem_attention_input_tensor);

  } else {
    //nothing
  }


  //////////////////////
  // IMAGE DECODER
  /////////////////////
  //current = ryouchinsa (works with ryouchinsa)
  //ailia:
  //input_name= [image_embeddings] ===> [1,256,64,64] <- image_encoder[vision_features]
  //input_name= [image_pe] ===> [1,256,64,64]
  //input_name= [sparse_prompt_embeddings] ===> [1,-1,256]
  //input_name= [dense_prompt_embeddings] ===> [1,256,64,64] 
  //input_name= [high_res_features1] ===> [1,32,256,256] <- image_encoder[backbone_fpn_0]
  //input_name= [high_res_features2] ===> [1,64,128,128] <- image_encoder[backbone_fpn_1]
  //output_name= [masks] ===> [1,-1,256,256]
  //output_name= [iou_pred] ===> [-1,4]
  //output_name= [sam_tokens_out] ===> [-1,-1,256]
  //output_name= [object_score_logits] ===> [-1,1]

  printf("/*********** image decoder ************/\n");
 
  //PREPARE INPUT TENSORS (image decoder)
  printf("Preparing input tensor (image decoder)...\n");
  std::vector<Ort::Value> img_decoder_input_tensor;
  
  //[vision_features]
  //work with a copy as we will use again in memory_encoder
  //int img_encoder_out_vision_features_idx = img_encoder.outputIdxByName("vision_features");
  //img_decoder_input_tensor.push_back(std::move(img_encoder_out[img_encoder_out_vision_features_idx]));    // vision_features
  //TensorCopy img_encoder_out_vision_features = setTensorCopy(std::move(img_encoder_out[img_encoder_out_vision_features_idx]));  
  if (frame_num == 0) {
    printf("\t (FIRST FRAME) Directly using img_encoder_out_vision_features.\n");
    img_decoder_input_tensor.push_back(getTensorCopy(inference_state.vision_features, memory_info));
  } else {
    printf("\t (NOT FIRST FRAME) Using pix_feat_nchw_for_decoder.\n");
    //NEW: To use the memory attention output
    img_decoder_input_tensor.push_back(getTensorCopy(pix_feat_nchw_for_decoder, memory_info));
  }

  //[image_pe]
  int prompt_encoder_out_dense_pe_idx = prompt_encoder.outputIdxByName(const_cast<char*>("dense_pe"));
  //img_decoder_input_tensor.push_back(std::move(prompt_encoder_out[prompt_encoder_out_dense_pe_idx]));    // image_embed
  img_decoder_input_tensor.push_back(getTensorCopy(inference_state.prompt_encoder_out_dense_pe, memory_info));

  //[sparse_prompt_embeddings]
  //int prompt_encoder_out_sparse_embeddings_idx = prompt_encoder.outputIdxByName("sparse_embeddings");
  //img_decoder_input_tensor.push_back(getTensorCopy(inference_state.prompt_encoder_out_sparse_embeddings, memory_info));

  //[dense_prompt_embeddings]
  //int prompt_encoder_out_dense_embeddings_idx = prompt_encoder.outputIdxByName("dense_embeddings");
  //img_decoder_input_tensor.push_back(getTensorCopy(inference_state.prompt_encoder_out_dense_embeddings, memory_info));
 
  if (frame_num == 0) {
    // Only feed prompts on first frame
    //[sparse_prompt_embeddings]
    img_decoder_input_tensor.push_back(getTensorCopy(inference_state.prompt_encoder_out_sparse_embeddings, memory_info));
    //[dense_prompt_embeddings]
    img_decoder_input_tensor.push_back(getTensorCopy(inference_state.prompt_encoder_out_dense_embeddings, memory_info));
  } else {
    // For later frames: feed empty tensors
    std::vector<int64_t> sparse_shape{1, 0, 256};  // [1,0,256]
    std::vector<int64_t> dense_shape{1, 256, 64, 64}; // fill with zeros
    std::vector<float> sparse_empty; // nothing
    std::vector<float> dense_empty(1 * 256 * 64 * 64, 0.0f);
    //[sparse_prompt_embeddings]
    img_decoder_input_tensor.push_back(
        Ort::Value::CreateTensor<float>(memory_info, sparse_empty.data(), sparse_empty.size(), sparse_shape.data(), sparse_shape.size()));
    //[dense_prompt_embeddings]
    img_decoder_input_tensor.push_back(
        Ort::Value::CreateTensor<float>(memory_info, dense_empty.data(), dense_empty.size(), dense_shape.data(), dense_shape.size()));
  }

  //[high_res_features1]
  //int img_encoder_out_high_res_features1_idx = img_encoder.outputIdxByName(const_cast<char*>("backbone_fpn_0"));
  //img_decoder_input_tensor.push_back(std::move(inference_state.img_encoder_out[img_encoder_out_high_res_features1_idx]));    // high_res_features1
  img_decoder_input_tensor.push_back(getTensorCopy(inference_state.backbone_fpn_0, memory_info));

  //[high_res_features2]
  //int img_encoder_out_high_res_features2_idx = img_encoder.outputIdxByName(const_cast<char*>("backbone_fpn_1")); 
  //img_decoder_input_tensor.push_back(std::move(inference_state.img_encoder_out[img_encoder_out_high_res_features2_idx]));    // high_res_features2
  img_decoder_input_tensor.push_back(getTensorCopy(inference_state.backbone_fpn_1, memory_info));

  //RUN INFERENCE (image decoder)
  printf("Inference (image decoder)...\n");
  std::vector<Ort::Value> img_decoder_out  = img_decoder.run(img_decoder_input_tensor);

  if (frame_num == 0) {
      //Compute the best mask 
      int img_decoder_out_iou_predictions_idx = img_decoder.outputIdxByName(const_cast<char*>("iou_pred")); 
      inference_state.maxScoreIdx = 0;
      float maxScore = 0;
      auto scoreShape = img_decoder_out[img_decoder_out_iou_predictions_idx].GetTensorTypeAndShapeInfo().GetShape();
      auto scoreValues = img_decoder_out[img_decoder_out_iou_predictions_idx].GetTensorMutableData<float>();
      int scoreNum = (int)scoreShape[1];
      for(int i = 0; i < scoreNum; i++){
        if(scoreValues[i] > maxScore){
          maxScore = scoreValues[i];
          inference_state.maxScoreIdx = i;
        }
      }
      printf("SELECTED MASK NUMBER %d\n", inference_state.maxScoreIdx);


  }
  int img_decoder_out_masks_idx = img_decoder.outputIdxByName(const_cast<char*>("masks"));
  TensorCopy img_decoder_out_masks = setTensorCopy(std::move(img_decoder_out[img_decoder_out_masks_idx]));  
  float* all_masks_ptr = getTensorCopy(img_decoder_out_masks, memory_info).GetTensorMutableData<float>();
  float* lowres_logits = all_masks_ptr + inference_state.maxScoreIdx*256*256;
  //cv::Mat low_res_mask_(256, 256, CV_32FC1, all_masks_ptr); //(pick the first)
  cv::Mat low_res_mask_(256, 256, CV_32FC1, lowres_logits); //(pick the best)

  // Build high-res mask for memory: logits → (sigmoid or binarize) → [1,1,1024,1024]
  std::vector<float> mask_for_mem; 
  bool binarize_from_points = !keep_memory; // set true if this mask came purely from point prompts and you want hard masks
  float sigmoid_scale = 1.0f, sigmoid_bias = 0.0f;
  make_mask_for_memory_from_logits(lowres_logits, 1024, mask_for_mem, binarize_from_points, sigmoid_scale, sigmoid_bias);

  if (keep_memory) {
      //////////////////////
      // MLP
      /////////////////////
      //input_name= [x] ===> [-1,256] ===> float32
      //output_name= [x_out] ===> [-1,256] ===> float32

      printf("/*********** mlp ************/\n");
     
      //PREPARE INPUT TENSORS (mlp)
      printf("Preparing input tensor (mlp)...\n");
      std::vector<Ort::Value> mlp_input_tensor;
      
      //[x] ===> [-1,256]
      int img_decoder_out_sam_tokens_out_idx = img_decoder.outputIdxByName(const_cast<char*>("sam_tokens_out"));
      TensorCopy img_decoder_out_sam_tokens_out = setTensorCopy(std::move(img_decoder_out[img_decoder_out_sam_tokens_out_idx]));
      //num elements = [1024]
      //shape = [[1,4,256]]
      //type = [float32]
      //need reshape [1,4,256] -> [4,256] (I guess, maybe would work with [1,256]?)
      TensorCopy img_decoder_out_sam_tokens_out_first = slice_1xNxC_toNxC(img_decoder_out_sam_tokens_out);
      //TODO: Also works if slice_1xNxC_toNxC, change if problems
      printTensorCopyInfo(img_decoder_out_sam_tokens_out_first);
      mlp_input_tensor.push_back(std::move(getTensorCopy(img_decoder_out_sam_tokens_out_first, memory_info)));    // x

       //RUN INFERENCE (mlp)
      printf("Inference (mlp)...\n");
      std::vector<Ort::Value> mlp_out  = mlp.run(mlp_input_tensor);

      //////////////////////
      // OBJ_PTR_TPOS_PROJ (used in version 2.1)
      /////////////////////
      //sam_tokens_out (256-d) ──> MLP ──> obj_ptr_tpos_proj_hiera ──> memory_2, memory_pos_2
      //input_name= [x] ===> [-1,256] ===> float32
      //output_name= [x_out] ===> [-1,64] ===> float32

      printf("/*********** obj_ptr_tpos_proj_hiera ************/\n");
     
      //PREPARE INPUT TENSORS (obj_ptr_tpos_proj_hiera)
      printf("Preparing input tensor (obj_ptr_tpos_proj_hiera)...\n");
      std::vector<Ort::Value> obj_ptr_tpos_proj_hiera_input_tensor;

      //[x] (will be x_out from MLP)
      int mlp_out_x_out_idx = mlp.outputIdxByName(const_cast<char*>("x_out"));
      obj_ptr_tpos_proj_hiera_input_tensor.push_back(std::move(mlp_out[mlp_out_x_out_idx]));    
      //img_decoder_input_tensor.push_back(getTensorCopy(inference_state.prompt_encoder_out_sparse_embeddings, memory_info));

       //RUN INFERENCE (obj_ptr_tpos_proj_hiera)
      printf("Inference (obj_ptr_tpos_proj_hiera)...\n");
      //std::vector<Ort::Value> obj_ptr_tpos_proj_hiera_out  = obj_ptr_tpos_proj_hiera.run(obj_ptr_tpos_proj_hiera_input_tensor);
      inference_state.obj_ptr_tpos_proj_hiera_out  = obj_ptr_tpos_proj_hiera.run(obj_ptr_tpos_proj_hiera_input_tensor);

      //SAVE THE RESULT
      //[x_out]
      //int obj_ptr_tpos_proj_hiera_out_x_out_idx = obj_ptr_tpos_proj_hiera.outputIdxByName("x_out");
      //inference_state.obj_ptr_tpos_proj_hiera_out_x_out = setTensorCopy(std::move(obj_ptr_tpos_proj_hiera_out[obj_ptr_tpos_proj_hiera_out_x_out_idx]));
     
      //We keep the result for the next iteration, it will be used in the memory attention.

      //////////////////////
      // MEM ENCODER
      /////////////////////
      //alia 
      //input_name= [pix_feat] ===> [1,256,64,64] ===> float32
      //input_name= [masks] ===> [1,1,1024,1024] ===> float32
      //output_name= [vision_features] ===> [1,64,64,64] ===> float32
      //output_name= [vision_pos_enc] ===> [1,64,64,64] ===> float32

      printf("/*********** mem_encoder **********/\n");
      
      //2) PREPARE INPUT TENSORS (mem_encoder)
      printf("Preparing input tensors (mem_encoder)...\n");
      std::vector<Ort::Value> mem_encoder_input_tensor;

      //[pix_feat] (TODO: this should be the attention output, but in aimol uses this)
      //int img_encoder_out_vision_features_idx = img_encoder.outputIdxByName("vision_features");
      //mem_encoder_input_tensor.push_back(std::move(img_encoder_out[img_encoder_out_vision_features_idx])); 
      //mem_encoder_input_tensor.push_back(std::move(getTensorCopy(img_encoder_out_vision_features, memory_info))); 
      

      //mem_encoder_input_tensor.push_back(getTensorCopy(inference_state.vision_features, memory_info));
      
      /*if (frame_num == 0) {
        mem_encoder_input_tensor.push_back(getTensorCopy(img_encoder_out_vision_features, memory_info));
      } else {
        //For frames>0 the input of the memory encoder is the output of the memory attention 
        mem_encoder_input_tensor.push_back(getTensorCopy(pix_feat_nchw_for_decoder, memory_info));
      }*/
      if (frame_num == 0)
        mem_encoder_input_tensor.push_back(getTensorCopy(inference_state.vision_features, memory_info));
      else
        mem_encoder_input_tensor.push_back(getTensorCopy(pix_feat_nchw_for_decoder, memory_info));

      //[masks] (We work with a copy as we will use img_decoder_out_masks later)
      //the original mask is upscaled (with a function called interpolated)
      
      

      ////cv::Mat low_res_mask; //TODO: Check if this is necessary
      ////low_res_mask_.convertTo(low_res_mask, CV_8UC1, 255);

      /*
      //My way
      cv::Mat high_res_mask;
      cv::Size sam2_image_size = cv::Size(1024, 1024);
      cv::resize(low_res_mask_, high_res_mask, sam2_image_size, 0, 0, cv::INTER_LINEAR);
      //need to use the preprocess function to transform into a tensor)
      std::vector<cv::Mat> input_masks;
      preprocess(high_res_mask, input_masks); 
      
      //mem_encoder_input_tensor.push_back(getTensorCopy(img_decoder_out_masks, memory_info));
      mem_encoder_input_tensor.push_back(std::move(Ort::Value::CreateTensor<float>(
          memory_info,
          input_masks[0].ptr<float>(),
          input_masks[0].total(),
          mem_encoder.inputs[mem_encoder.inputIdxByName("masks")].shape.data(),
          mem_encoder.inputs[mem_encoder.inputIdxByName("masks")].shape.size()))
      ); 
      */

      //ChatGPT way
      
      // Create an ORT tensor from our contiguous vector
      const int64_t mask_shape[4] = {1, 1, 1024, 1024};
      mem_encoder_input_tensor.push_back(
      Ort::Value::CreateTensor<float>(
        memory_info,
        mask_for_mem.data(),
        mask_for_mem.size(),
        mask_shape, 4)
      );


      //3) RUN INFERENCE (mem_encoder decoder)
      printf("Inference (mem_encoder)...\n");
      std::vector<Ort::Value> mem_encoder_out  = mem_encoder.run(mem_encoder_input_tensor);
      
      //////////////////////
      // STORE IN MEMORY
      //////////////////////
      /*
      Inputs

      From memory encoder:
      //output_name= [vision_features] ===> [1,64,64,64]
      //output_name= [vision_pos_enc] ===> [1,64,64,64]

      From MLP:
      [x_out] ===> [-1,256]

      The output should be used from the memory attention:

      //input_name= [curr] ===> [4096,1,256] ===> float32
      //input_name= [memory_1] ===> [-1,1,64] ===> float32
      //input_name= [memory_2] ===> [-1,1,64] ===> float32
      //input_name= [curr_pos] ===> [4096,1,256] ===> float32
      //input_name= [memory_pos_1] ===> [-1,1,64] ===> float32
      //input_name= [memory_pos_2] ===> [-1,1,64] ===> float32
      //input_name= [attention_mask_1] ===> [-1,1] ===> bool
      //input_name= [attention_mask_2] ===> [-1,1] ===> bool
      */

      /*
      //just debugging trunc_normal
      std::vector<size_t> shape = {2, 3, 4};  // 3D tensor (can be N-D)
      std::vector<float> values = trunc_normal(shape);

      size_t total_size = values.size();
      for (size_t i = 0; i < total_size; ++i) {
          std::cout << "values[" << i << "] = " << values[i] << "\n";
      }
      */

      /* 
        Chat GPT attempt:
      */
      // 5) Store in our memory bank for later memory-attention
      int mem_feat_idx = mem_encoder.outputIdxByName(const_cast<char*>("vision_features")); // [1,64,64,64]
      int mem_pos_idx  = mem_encoder.outputIdxByName(const_cast<char*>("vision_pos_enc"));  // [1,64,64,64]

      TensorCopy maskmem_features = setTensorCopy(std::move(mem_encoder_out[mem_feat_idx]));
      TensorCopy maskmem_posenc  = setTensorCopy(std::move(mem_encoder_out[mem_pos_idx]));

      inference_state.maskmem_features_BC64x64.push_back(std::move(maskmem_features));
      inference_state.maskmem_posenc_BC64x64.push_back(std::move(maskmem_posenc));

      //if you never pop old ones, the bank will grow forever. You may need to keep only the latest N frames (like SAM2’s "sliding memory"). Example:
      const int MAX_MEM = 8; // tune
      if (inference_state.maskmem_features_BC64x64.size() > MAX_MEM) {
        inference_state.maskmem_features_BC64x64.erase(inference_state.maskmem_features_BC64x64.begin());
        inference_state.maskmem_posenc_BC64x64.erase(inference_state.maskmem_posenc_BC64x64.begin());
      }
  }

  /////////////////
  // POSTPROCESS
  /////////////////
  printf("Postprocessing...\n");

  //TODO per que no em cal saber quina és la millor??
  /*int img_decoder_out_iou_predictions_idx = img_decoder.outputIdxByName("iou_pred"); 
  int maxScoreIdx = 0;
  float maxScore = 0;
  auto scoreShape = img_decoder_out[img_decoder_out_iou_predictions_idx].GetTensorTypeAndShapeInfo().GetShape();
  auto scoreValues = img_decoder_out[img_decoder_out_iou_predictions_idx].GetTensorMutableData<float>();
  int scoreNum = (int)scoreShape[1];
  for(int i = 0; i < scoreNum; i++){
    printf("Checking mask scores number %d = %f\n", i, maxScore);
    if(scoreValues[i] > maxScore){
      maxScore = scoreValues[i];
      maxScoreIdx = i;
    }
  }*/

  //As we use img_decoder_out_masks twice (before in the memory encoder) we need a copy. 
  //float* maskValues = img_decoder_out[img_decoder_out_masks_idx].GetTensorMutableData<float>();
  //float* maskValues = getTensorCopy(img_decoder_out_masks, memory_info).GetTensorMutableData<float>();

  //original image size (ori_img_cols = image.size[1], ori_img_rows = image.size[0])
  std::vector<int64_t> orig_im_size_values_int64 = {ori_img_rows, ori_img_cols};
  
  
  cv::Mat maskProb1024(1024, 1024, CV_32FC1, mask_for_mem.data());
  cv::Mat maskProbOrig;
  cv::resize(maskProb1024, maskProbOrig, image.size(), 0, 0, cv::INTER_LINEAR);

  ///*
  //Display nice

  cv::Mat overlayed = overlayMask(image, maskProbOrig, cv::Scalar(0,255,0), 0.4);

  drawPromptPoints(overlayed, promptPoints, image);
  //if (display) {
  //  cv::imshow("segmentation", overlayed);
  //  cv::waitKey(0);
  //}
  
  std::ostringstream ss;
  ss << "output/debug_frame_" << frame_num << ".png";
  cv::imwrite(ss.str(), overlayed);
  //*/

  printf("Postprocessing DONE.\n");
  

  //cv::imwrite("/Users/rtous/dev/back2black/borrar_sam21_lowres_logits.png", lowres_logits); 
  
  cv::Mat mask_0_255;
  maskProbOrig.convertTo(mask_0_255, CV_8UC1, 255.0); // scale 0–1 → 0–255
  //cv::imwrite("/Users/rtous/dev/back2black/output/borrar_sam21_maskProbOrig_from_mask_for_mem.png", mask_0_255); 

  return mask_0_255;
  //return maskProbOrig;

}

void SAM21::test()
{
 
  printf("DIRECT TEST (without Segmentor/Segmentor21\n");

  SAM21 sam21("checkpoints/ailia_sam21_tiny", "t");

  printf("Reading video...\n");
  //std::string video_path = "demo_ailia.mp4";
  std::string video_path = "footage.mp4";
  cv::VideoCapture capture(video_path);
  if (!capture.isOpened()) return;

  std::vector<PromptPoint> promptPoints = {
    //{550.0f, 250.0f, 1},  // tshirt
    {485.0f, 355.0f, 1},  // ball
  };
  printf("TRACKING POINT %d, %d\n", promptPoints[0].x, promptPoints[0].y);

  //prompt
  /*std::vector<float> inputPointValues;
  std::vector<int> inputLabelValues;
  inputPointValues.push_back((float)480);  
  inputPointValues.push_back((float)350); 
  inputLabelValues.push_back(1);*/

  cv::Mat frame;
  int i = 0;
  while (true) {
      printf("\n**************************\n");
      printf("Frame %d\n", i);
      printf("**************************\n");
      if (!capture.read(frame) || frame.empty()) break;
      printf("Inferencing frame...\n");
      //auto result = sam2->inference(frame);
      sam21.image_encoder(frame);


      sam21.inference_frame(frame, i, promptPoints, true, false);
      

      i++;
      printf("Frame 1 DONE.\n");
  }
  capture.release();
}

/*int main()
{
  test();
}*/